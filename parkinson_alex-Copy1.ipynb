{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba52e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c963e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 15 04:44:42 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.76       Driver Version: 496.76       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P8     6W /  N/A |    134MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f27747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from imutils import paths\n",
    "import random\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da7d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"Park\"\n",
    "\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
    "VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae30521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(\"Dataset\"))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# compute the training and testing split\n",
    "i = int(len(imagePaths) * 0.85)\n",
    "trainPaths = imagePaths[:i]\n",
    "valPaths = imagePaths[i:]\n",
    "\n",
    "# define the datasets that we'll be building\n",
    "datasets = [\n",
    "\t(\"training\", trainPaths, TRAIN_PATH),\n",
    "\t(\"validation\", valPaths, VAL_PATH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d34c11b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n",
      "Dataset\n"
     ]
    }
   ],
   "source": [
    "# for (dType, imagePaths, baseOutput) in datasets:\n",
    "# \t# show which data split we are creating\n",
    "# \tprint(\"[INFO] building '{}' split\".format(dType))\n",
    "\n",
    "\t# if the output base output directory does not exist, create it\n",
    "# \tif not os.path.exists(baseOutput):\n",
    "# \t\tprint(\"[INFO] 'creating {}' directory\".format(baseOutput))\n",
    "# \t\tos.makedirs(baseOutput)\n",
    "\n",
    "\t# loop over the input image paths\n",
    "for inputPath in imagePaths:\n",
    "    # extract the filename of the input image along with its\n",
    "    # corresponding class label\n",
    "    filename = inputPath.split(os.path.sep)[-1]\n",
    "    print(inputPath.split(os.path.sep)[-3])\n",
    "# \t\tlabel = inputPath.split(os.path.sep)[-2]\n",
    "\n",
    "# \t\t# build the path to the label directory\n",
    "# \t\tlabelPath = os.path.sep.join([baseOutput, label])\n",
    "\n",
    "# \t\t# if the label output directory does not exist, create it\n",
    "# \t\tif not os.path.exists(labelPath):\n",
    "# \t\t\tprint(\"[INFO] 'creating {}' directory\".format(labelPath))\n",
    "# \t\t\tos.makedirs(labelPath)\n",
    "\n",
    "# \t\t# construct the path to the destination image and then copy\n",
    "# \t\t# the image itself\n",
    "# \t\tp = os.path.sep.join([labelPath, filename])\n",
    "# \t\tshutil.copy2(inputPath, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de147f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.image.ImageDataGenerator object at 0x000001C67327ACD0>\n",
      "<keras.preprocessing.image.ImageDataGenerator object at 0x000001C67327AB80>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "trainAug = ImageDataGenerator(\n",
    "\trotation_range=25,\n",
    "\tzoom_range=0.1,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.2,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")\n",
    "print(trainAug)\n",
    "valAug = ImageDataGenerator()\n",
    "print(valAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2527135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6813 images belonging to 2 classes.\n",
      "Found 1203 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "BS=64\n",
    "\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "\tTRAIN_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(256, 256),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=True,\n",
    "\tbatch_size=BS)\n",
    "\n",
    "# initialize the validation generator\n",
    "valGen = valAug.flow_from_directory(\n",
    "\tVAL_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(256, 256),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=False,\n",
    "\tbatch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81427f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b52ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Dropout, GlobalAveragePooling2D,AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.layers import BatchNormalization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1bb8ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 62, 62, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 30, 30, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 30, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 384)       1327488   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 14, 14, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 256)       884992    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 14, 14, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,295,042\n",
      "Trainable params: 58,292,290\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# alex imple.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(256,256,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "#Model Summary\n",
    "optimizer = Adam(0.0001)\n",
    "# optimizer = SGD(learning_rate=0.0001, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22e9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelalex = np.load('bvlc_alexnet.npy',allow_pickle=True)\n",
    "# modelalex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11fdf1e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 62, 62, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 30, 30, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 30, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 384)       1327488   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 14, 14, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 256)       884992    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 14, 14, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,295,042\n",
      "Trainable params: 58,292,290\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded = tf.keras.models.load_model('park_best.h5')\n",
    "loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b40c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=3, min_lr=1e-5, mode='min', verbose=1)\n",
    "model_chkpt = ModelCheckpoint('park_best.h5',save_best_only=True, monitor='val_loss',mode='min', verbose=1)\n",
    "\n",
    "callback_list = [model_chkpt, lr_reduce]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c3df9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.6858 - accuracy: 0.5418\n",
      "Epoch 00001: val_loss improved from inf to 1.37295, saving model to park_best.h5\n",
      "107/107 [==============================] - 104s 885ms/step - loss: 1.6858 - accuracy: 0.5418 - val_loss: 1.3730 - val_accuracy: 0.4929 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.9797 - accuracy: 0.5490\n",
      "Epoch 00002: val_loss improved from 1.37295 to 0.63241, saving model to park_best.h5\n",
      "107/107 [==============================] - 94s 878ms/step - loss: 0.9797 - accuracy: 0.5490 - val_loss: 0.6324 - val_accuracy: 0.6509 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.8268 - accuracy: 0.5644\n",
      "Epoch 00003: val_loss did not improve from 0.63241\n",
      "107/107 [==============================] - 87s 813ms/step - loss: 0.8268 - accuracy: 0.5644 - val_loss: 0.6834 - val_accuracy: 0.5736 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7528 - accuracy: 0.5782\n",
      "Epoch 00004: val_loss did not improve from 0.63241\n",
      "107/107 [==============================] - 86s 806ms/step - loss: 0.7528 - accuracy: 0.5782 - val_loss: 0.7159 - val_accuracy: 0.5653 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7210 - accuracy: 0.5820\n",
      "Epoch 00005: val_loss improved from 0.63241 to 0.62682, saving model to park_best.h5\n",
      "107/107 [==============================] - 87s 809ms/step - loss: 0.7210 - accuracy: 0.5820 - val_loss: 0.6268 - val_accuracy: 0.6692 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.6082\n",
      "Epoch 00006: val_loss improved from 0.62682 to 0.62554, saving model to park_best.h5\n",
      "107/107 [==============================] - 88s 817ms/step - loss: 0.6894 - accuracy: 0.6082 - val_loss: 0.6255 - val_accuracy: 0.6409 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6704 - accuracy: 0.6156\n",
      "Epoch 00007: val_loss did not improve from 0.62554\n",
      "107/107 [==============================] - 90s 836ms/step - loss: 0.6704 - accuracy: 0.6156 - val_loss: 0.6778 - val_accuracy: 0.5736 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6537 - accuracy: 0.6303\n",
      "Epoch 00008: val_loss did not improve from 0.62554\n",
      "107/107 [==============================] - 85s 798ms/step - loss: 0.6537 - accuracy: 0.6303 - val_loss: 0.6600 - val_accuracy: 0.6068 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6444 - accuracy: 0.6454\n",
      "Epoch 00009: val_loss improved from 0.62554 to 0.58711, saving model to park_best.h5\n",
      "107/107 [==============================] - 87s 810ms/step - loss: 0.6444 - accuracy: 0.6454 - val_loss: 0.5871 - val_accuracy: 0.6825 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6401 - accuracy: 0.6473\n",
      "Epoch 00010: val_loss improved from 0.58711 to 0.55522, saving model to park_best.h5\n",
      "107/107 [==============================] - 87s 807ms/step - loss: 0.6401 - accuracy: 0.6473 - val_loss: 0.5552 - val_accuracy: 0.7265 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6185 - accuracy: 0.6568\n",
      "Epoch 00011: val_loss did not improve from 0.55522\n",
      "107/107 [==============================] - 85s 792ms/step - loss: 0.6185 - accuracy: 0.6568 - val_loss: 0.5662 - val_accuracy: 0.6983 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5917 - accuracy: 0.6865\n",
      "Epoch 00012: val_loss did not improve from 0.55522\n",
      "107/107 [==============================] - 85s 795ms/step - loss: 0.5917 - accuracy: 0.6865 - val_loss: 0.5621 - val_accuracy: 0.7157 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5960 - accuracy: 0.6935\n",
      "Epoch 00013: val_loss improved from 0.55522 to 0.54015, saving model to park_best.h5\n",
      "107/107 [==============================] - 87s 808ms/step - loss: 0.5960 - accuracy: 0.6935 - val_loss: 0.5401 - val_accuracy: 0.7249 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.6994\n",
      "Epoch 00014: val_loss did not improve from 0.54015\n",
      "107/107 [==============================] - 85s 792ms/step - loss: 0.5844 - accuracy: 0.6994 - val_loss: 0.5856 - val_accuracy: 0.6692 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.6973\n",
      "Epoch 00015: val_loss did not improve from 0.54015\n",
      "107/107 [==============================] - 85s 796ms/step - loss: 0.5748 - accuracy: 0.6973 - val_loss: 0.7884 - val_accuracy: 0.5869 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5625 - accuracy: 0.7120\n",
      "Epoch 00016: val_loss improved from 0.54015 to 0.51123, saving model to park_best.h5\n",
      "107/107 [==============================] - 85s 796ms/step - loss: 0.5625 - accuracy: 0.7120 - val_loss: 0.5112 - val_accuracy: 0.7473 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5564 - accuracy: 0.7098\n",
      "Epoch 00017: val_loss did not improve from 0.51123\n",
      "107/107 [==============================] - 84s 785ms/step - loss: 0.5564 - accuracy: 0.7098 - val_loss: 0.5165 - val_accuracy: 0.7273 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.7301\n",
      "Epoch 00018: val_loss did not improve from 0.51123\n",
      "107/107 [==============================] - 84s 785ms/step - loss: 0.5409 - accuracy: 0.7301 - val_loss: 0.9283 - val_accuracy: 0.5744 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.7327\n",
      "Epoch 00019: val_loss did not improve from 0.51123\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "107/107 [==============================] - 84s 783ms/step - loss: 0.5294 - accuracy: 0.7327 - val_loss: 0.6991 - val_accuracy: 0.6151 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.4957 - accuracy: 0.7530\n",
      "Epoch 00020: val_loss improved from 0.51123 to 0.46354, saving model to park_best.h5\n",
      "107/107 [==============================] - 85s 794ms/step - loss: 0.4957 - accuracy: 0.7530 - val_loss: 0.4635 - val_accuracy: 0.7697 - lr: 4.0000e-05\n",
      "Epoch 21/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.4633 - accuracy: 0.7748\n",
      "Epoch 00021: val_loss improved from 0.46354 to 0.41912, saving model to park_best.h5\n",
      "107/107 [==============================] - 85s 798ms/step - loss: 0.4633 - accuracy: 0.7748 - val_loss: 0.4191 - val_accuracy: 0.8022 - lr: 4.0000e-05\n",
      "Epoch 22/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.7860\n",
      "Epoch 00022: val_loss did not improve from 0.41912\n",
      "107/107 [==============================] - 84s 782ms/step - loss: 0.4477 - accuracy: 0.7860 - val_loss: 0.4258 - val_accuracy: 0.7914 - lr: 4.0000e-05\n",
      "Epoch 23/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.7812\n",
      "Epoch 00023: val_loss did not improve from 0.41912\n",
      "107/107 [==============================] - 84s 781ms/step - loss: 0.4536 - accuracy: 0.7812 - val_loss: 0.4831 - val_accuracy: 0.7697 - lr: 4.0000e-05\n",
      "Epoch 24/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.4278 - accuracy: 0.8026\n",
      "Epoch 00024: val_loss did not improve from 0.41912\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5999999595806004e-05.\n",
      "107/107 [==============================] - 84s 781ms/step - loss: 0.4278 - accuracy: 0.8026 - val_loss: 0.6208 - val_accuracy: 0.7066 - lr: 4.0000e-05\n",
      "Epoch 25/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.4120 - accuracy: 0.8046\n",
      "Epoch 00025: val_loss improved from 0.41912 to 0.38959, saving model to park_best.h5\n",
      "107/107 [==============================] - 85s 792ms/step - loss: 0.4120 - accuracy: 0.8046 - val_loss: 0.3896 - val_accuracy: 0.8188 - lr: 1.6000e-05\n",
      "Epoch 26/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8178\n",
      "Epoch 00026: val_loss improved from 0.38959 to 0.38557, saving model to park_best.h5\n",
      "107/107 [==============================] - 85s 794ms/step - loss: 0.3914 - accuracy: 0.8178 - val_loss: 0.3856 - val_accuracy: 0.8105 - lr: 1.6000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3871 - accuracy: 0.8187\n",
      "Epoch 00027: val_loss did not improve from 0.38557\n",
      "107/107 [==============================] - 85s 792ms/step - loss: 0.3871 - accuracy: 0.8187 - val_loss: 0.4877 - val_accuracy: 0.7490 - lr: 1.6000e-05\n",
      "Epoch 28/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3799 - accuracy: 0.8211\n",
      "Epoch 00028: val_loss did not improve from 0.38557\n",
      "107/107 [==============================] - 87s 809ms/step - loss: 0.3799 - accuracy: 0.8211 - val_loss: 0.4665 - val_accuracy: 0.7789 - lr: 1.6000e-05\n",
      "Epoch 29/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3874 - accuracy: 0.8192\n",
      "Epoch 00029: val_loss improved from 0.38557 to 0.35965, saving model to park_best.h5\n",
      "107/107 [==============================] - 85s 796ms/step - loss: 0.3874 - accuracy: 0.8192 - val_loss: 0.3596 - val_accuracy: 0.8254 - lr: 1.6000e-05\n",
      "Epoch 30/30\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3670 - accuracy: 0.8337\n",
      "Epoch 00030: val_loss did not improve from 0.35965\n",
      "107/107 [==============================] - 84s 781ms/step - loss: 0.3670 - accuracy: 0.8337 - val_loss: 0.3638 - val_accuracy: 0.8213 - lr: 1.6000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a9cf08d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainGen,\n",
    "          validation_data = valGen,\n",
    "          epochs=30,\n",
    "          callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93de3576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 3s 167ms/step - loss: 0.3596 - accuracy: 0.8254\n",
      "Restored model, accuracy: 82.54%\n"
     ]
    }
   ],
   "source": [
    "bestmodel = model.load_weights('C:\\\\Users\\\\\\kksat\\\\Desktop\\\\mini project\\\\park_best.h5')\n",
    "loss, acc = model.evaluate(valGen)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "444f1141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.8297\n",
      "Epoch 00001: val_loss did not improve from 0.35965\n",
      "107/107 [==============================] - 89s 824ms/step - loss: 0.3698 - accuracy: 0.8297 - val_loss: 0.3910 - val_accuracy: 0.8063 - lr: 1.6000e-05\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3614 - accuracy: 0.8316\n",
      "Epoch 00002: val_loss improved from 0.35965 to 0.33789, saving model to park_best.h5\n",
      "107/107 [==============================] - 86s 804ms/step - loss: 0.3614 - accuracy: 0.8316 - val_loss: 0.3379 - val_accuracy: 0.8446 - lr: 1.6000e-05\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.8415\n",
      "Epoch 00003: val_loss did not improve from 0.33789\n",
      "107/107 [==============================] - 88s 820ms/step - loss: 0.3528 - accuracy: 0.8415 - val_loss: 0.4030 - val_accuracy: 0.8038 - lr: 1.6000e-05\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8402\n",
      "Epoch 00004: val_loss did not improve from 0.33789\n",
      "107/107 [==============================] - 88s 824ms/step - loss: 0.3508 - accuracy: 0.8402 - val_loss: 0.3978 - val_accuracy: 0.8121 - lr: 1.6000e-05\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.8410\n",
      "Epoch 00005: val_loss did not improve from 0.33789\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "107/107 [==============================] - 89s 829ms/step - loss: 0.3517 - accuracy: 0.8410 - val_loss: 0.3418 - val_accuracy: 0.8379 - lr: 1.6000e-05\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.8416\n",
      "Epoch 00006: val_loss did not improve from 0.33789\n",
      "107/107 [==============================] - 85s 795ms/step - loss: 0.3426 - accuracy: 0.8416 - val_loss: 0.3533 - val_accuracy: 0.8346 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3361 - accuracy: 0.8475\n",
      "Epoch 00007: val_loss did not improve from 0.33789\n",
      "107/107 [==============================] - 88s 818ms/step - loss: 0.3361 - accuracy: 0.8475 - val_loss: 0.4148 - val_accuracy: 0.8105 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.8601\n",
      "Epoch 00008: val_loss did not improve from 0.33789\n",
      "107/107 [==============================] - 87s 814ms/step - loss: 0.3196 - accuracy: 0.8601 - val_loss: 0.3895 - val_accuracy: 0.8229 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3248 - accuracy: 0.8563\n",
      "Epoch 00009: val_loss improved from 0.33789 to 0.32292, saving model to park_best.h5\n",
      "107/107 [==============================] - 87s 812ms/step - loss: 0.3248 - accuracy: 0.8563 - val_loss: 0.3229 - val_accuracy: 0.8454 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8548\n",
      "Epoch 00010: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 85s 796ms/step - loss: 0.3220 - accuracy: 0.8548 - val_loss: 0.3466 - val_accuracy: 0.8446 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a955a7460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainGen,\n",
    "          validation_data = valGen,\n",
    "          epochs=10,\n",
    "          callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cfffc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 3s 138ms/step - loss: 0.3229 - accuracy: 0.8454\n",
      "Restored model, accuracy: 84.54%\n"
     ]
    }
   ],
   "source": [
    "bestmodel = model.load_weights('C:\\\\Users\\\\\\kksat\\\\Desktop\\\\mini project\\\\park_best.h5')\n",
    "loss, acc = model.evaluate(valGen)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c467273e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3235 - accuracy: 0.8540\n",
      "Epoch 00001: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 98s 911ms/step - loss: 0.3235 - accuracy: 0.8540 - val_loss: 0.3813 - val_accuracy: 0.8221 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8565\n",
      "Epoch 00002: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 95s 887ms/step - loss: 0.3175 - accuracy: 0.8565 - val_loss: 0.4142 - val_accuracy: 0.8063 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8613\n",
      "Epoch 00003: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 94s 876ms/step - loss: 0.3079 - accuracy: 0.8613 - val_loss: 0.3628 - val_accuracy: 0.8213 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3161 - accuracy: 0.8576\n",
      "Epoch 00004: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 94s 877ms/step - loss: 0.3161 - accuracy: 0.8576 - val_loss: 0.6382 - val_accuracy: 0.7140 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.8648\n",
      "Epoch 00005: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 90s 843ms/step - loss: 0.3001 - accuracy: 0.8648 - val_loss: 0.4560 - val_accuracy: 0.7839 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.8654\n",
      "Epoch 00006: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 90s 840ms/step - loss: 0.3051 - accuracy: 0.8654 - val_loss: 0.3988 - val_accuracy: 0.8121 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.8694\n",
      "Epoch 00007: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 89s 831ms/step - loss: 0.3003 - accuracy: 0.8694 - val_loss: 0.3783 - val_accuracy: 0.8279 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.8680\n",
      "Epoch 00008: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 89s 828ms/step - loss: 0.2950 - accuracy: 0.8680 - val_loss: 0.4051 - val_accuracy: 0.8146 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8657\n",
      "Epoch 00009: val_loss did not improve from 0.32292\n",
      "107/107 [==============================] - 89s 831ms/step - loss: 0.2990 - accuracy: 0.8657 - val_loss: 0.3931 - val_accuracy: 0.8288 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.8701\n",
      "Epoch 00010: val_loss improved from 0.32292 to 0.29330, saving model to park_best.h5\n",
      "107/107 [==============================] - 90s 844ms/step - loss: 0.2963 - accuracy: 0.8701 - val_loss: 0.2933 - val_accuracy: 0.8712 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b4b5e99d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainGen,\n",
    "          validation_data = valGen,\n",
    "          epochs=10,\n",
    "          callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff41e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valGen.reset()\n",
    "# model.evaluate(valGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5215b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 3s 164ms/step - loss: 0.2933 - accuracy: 0.8712\n",
      "Restored model, accuracy: 87.12%\n"
     ]
    }
   ],
   "source": [
    "bestmodel = model.load_weights('C:\\\\Users\\\\\\kksat\\\\Desktop\\\\mini project\\\\park_best.h5')\n",
    "loss, acc = model.evaluate(valGen)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe56b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae69b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
